# Gradient descent algorithm for dem_12_10.csv datasets

using DataFrames
using Plots
using GLM
using CSV

# import dem_12_10.csv datasets
dem_12_10 = CSV.read("datasets/dem_12_10.csv", DataFrame)
dem_12_10 = dem_12_10[:, 2:3]
describe(dem_12_10)

# Exact solution
dem_12_10_lm = lm(
	@formula(Giveny12_10~1+Givenx12_10+Givenx12_10^2+Givenx12_10^3), dem_12_10
)

# the solution space is ð’® âˆˆ â„â´
subspace_sol = -5:0.001:5
egsol = rand(subspace_sol, 4)
exactsol = [0.97619, 5.21429, -2.22619, 0.25000]
	
function y_hat(x_input, reg_coef::Vector{Float64})
	estimated_y = reg_coef[1] * (x_input .^0) +
		reg_coef[2] * (x_input .^1) +
		reg_coef[3] * (x_input .^2) +
		reg_coef[4] * (x_input .^3)
	return estimated_y
end

y_hat(dem_12_10[:,1], egsol)

# an example the computation of âˆ‚SSE/âˆ‚Î²â‚—
# âˆ‚SSE/âˆ‚Î²â‚— = âˆ’2âˆ‘(xË¡áµ¢yáµ¢ - âˆ‘Î²â±¼xÊ²áµ¢xË¡áµ¢)
#
# First, for l = 0, then the computation of âˆ‚SSE/âˆ‚Î²â‚—for the egsol is
# computed as follows
# first_term = dem_12_10[:,1] .^0 .* dem_12_10[:,2]
# second_term = egsol[1] * (dem_12_10[:,1] .^0 .* dem_12_10[:,1] .^0) +
#	egsol[2] * (dem_12_10[:,1] .^1 .* dem_12_10[:,1] .^0) +
#	egsol[3] * (dem_12_10[:,1] .^2 .* dem_12_10[:,1] .^0) +
#	egsol[4] * (dem_12_10[:,1] .^3 .* dem_12_10[:,1] .^0)

# -2 * sum(first_term - second_term)

# First, for l = 0, then the computation of âˆ‚SSE/âˆ‚Î²â‚—for the exactsol is
# computed as follows
#first_term = dem_12_10[:,1] .^0 .* dem_12_10[:,2]
#second_term = exactsol[1] * (dem_12_10[:,1] .^0 .* dem_12_10[:,1] .^0) +
#	exactsol[2] * (dem_12_10[:,1] .^1 .* dem_12_10[:,1] .^0) +
#	exactsol[3] * (dem_12_10[:,1] .^2 .* dem_12_10[:,1] .^0) +
#	exactsol[4] * (dem_12_10[:,1] .^3 .* dem_12_10[:,1] .^0)

#-2 * sum(first_term - second_term)

function partial_sse(x_input, y_output, beta_l::Int64, reg_coef::Vector{Float64})
	first_term = x_input .^beta_l .* y_output
	second_term = reg_coef[1] * (x_input .^0 .* x_input .^beta_l) +
		reg_coef[2] * (x_input .^1 .* x_input .^beta_l) +
		reg_coef[3] * (x_input .^2 .* x_input .^beta_l) +
		reg_coef[4] * (x_input .^3 .* x_input .^beta_l)
	âˆ‚SSE_âˆ‚Î²â‚— = -2 * sum(first_term - second_term)
	return âˆ‚SSE_âˆ‚Î²â‚—
end

sum(partial_sse(dem_12_10[:,1], dem_12_10[:,2], 1, egsol).^2)
sum(partial_sse(dem_12_10[:,1], dem_12_10[:,2], 1, exactsol).^2)

# Compute the gradient of SSE, i.e., âˆ‡SSE â‰¡ âˆ‡Ï•(ð›ƒ)
# âˆ‡Ï•(ð›ƒ) = (âˆ‚Ï•/âˆ‚Î²â‚€, âˆ‚Ï•/âˆ‚Î²â‚, â€¦ , âˆ‚Ï•/âˆ‚Î²â‚–)
function âˆ‡SSE(x_input, y_output, reg_coef::Vector{Float64})
	sse_vect = []
	nâƒ— = 0:length(reg_coef)-1
	for k âˆˆ nâƒ—
		push!(sse_vect, partial_sse(x_input, y_output, k, reg_coef))
	end
	return sse_vect
end

sum(âˆ‡SSE(dem_12_10[:,1], dem_12_10[:,2], exactsol).^2)
sum(âˆ‡SSE(dem_12_10[:,1], dem_12_10[:,2], egsol).^2)

# Compute the gradient descent
function gradient_descent(x_input, y_output, Î±::Float64)
	b = rand(subspace_sol, 4)
	b_vect = []
	push!(b_vect, b)
	while sum(b.^2) > 0.001
		b = b .- Î± * âˆ‡SSE(x_input, y_output, b)
		push!(b_vect, b)
	end
	return b_vect
end

begin
	MAX_ITERATION = 10^3
	Î¼âƒ— = gradient_descent(dem_12_10[:,1], dem_12_10[:,2], 0.000323)
end

